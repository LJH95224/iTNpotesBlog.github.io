(window.webpackJsonp=window.webpackJsonp||[]).push([[189],{521:function(t,a,s){"use strict";s.r(a);var r=s(42),v=Object(r.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"大文件上传和断点续传"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#大文件上传和断点续传"}},[t._v("#")]),t._v(" 大文件上传和断点续传")]),t._v(" "),s("h2",{attrs:{id:"大文件上传"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#大文件上传"}},[t._v("#")]),t._v(" 大文件上传")]),t._v(" "),s("h3",{attrs:{id:"前端"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前端"}},[t._v("#")]),t._v(" 前端")]),t._v(" "),s("p",[t._v("前端大文件上传网上的大部分文章已经给出了解决方案，核心是利用 "),s("strong",[t._v("Blob.prototype.slice")]),t._v(" 方法，此方法和数组的 "),s("strong",[t._v("slice")]),t._v(" 方法相似，调用的 slice 方法可以返回原文件的某个切片。")]),t._v(" "),s("p",[t._v("这样我们就可以根据预先设置好的切片最大数量将文件切分为一个个切片，然后借助 http 的可并发性，同时上传多个切片，这样从原本传一个大文件，变成了同时传多个小的文件切片，可以大大减少上传时间。")]),t._v(" "),s("p",[t._v("另外由于是并发，传输到服务端的顺序可能会发生变化，所以我们还需要给每个切片记录顺序。")]),t._v(" "),s("h3",{attrs:{id:"服务端"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#服务端"}},[t._v("#")]),t._v(" 服务端")]),t._v(" "),s("p",[t._v("服务端需要负责接受这些切片，并在接收到所有切片后合并切片。")]),t._v(" "),s("p",[t._v("这里又引申出两个问题：")]),t._v(" "),s("ol",[s("li",[t._v("何时合并切片，即切片什么时候传输完成？")]),t._v(" "),s("li",[t._v("如何合并切片？")])]),t._v(" "),s("p",[t._v("第一个问题需要前端进行配合，前端在每个切片中都携带切片最大数量的信息，当服务端接收到这个数量的切片时自动合并，也可以额外发一个请求主动通知服务端进行切片的合并。")]),t._v(" "),s("p",[t._v("第二个问题，具体如何合并切片呢？这里可以使用 NodeJS 的 API "),s("strong",[t._v("fs.appendFileSync")]),t._v("，它可以同步地将数据追加到指定文件，也就是说，当服务端接收完所有切片后，可以先创建一个空文件，然后将所有切片逐步合并到这个文件中。")]),t._v(" "),s("h2",{attrs:{id:"前端部分"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前端部分"}},[t._v("#")]),t._v(" 前端部分")])])}),[],!1,null,null,null);a.default=v.exports}}]);